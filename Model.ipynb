{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navigating a RaceTrack"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pygame\n",
    "import time\n",
    "import matplotlib.pyplot as plt #For plotting state-action values, Code for this has been commented out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment and Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    def __init__(self):\n",
    "        \n",
    "        # The track has been harcoded Manually\n",
    "        \"\"\"Create Track:\n",
    "            -Valid Road: 0\n",
    "            -Wall: -1\n",
    "            -Finish Line: +1 \"\"\"\n",
    "        self.track_info = np.zeros((8,21))\n",
    "        for i in range(self.track_info.shape[1]):\n",
    "            self.track_info[0,i] = -1\n",
    "        for i in range(0,8):\n",
    "            self.track_info[1,i] = -1\n",
    "        self.track_info[1,20] = -1\n",
    "        for i in range(2,6):\n",
    "            self.track_info[i,0] = -1\n",
    "            self.track_info[i,20] = -1\n",
    "        self.track_info[3,15] = -1\n",
    "        self.track_info[3,16] = -1\n",
    "        self.track_info[3,17] = -1\n",
    "        for i in range(9,14):\n",
    "            self.track_info[4,i] = -1\n",
    "        self.track_info[4,19] = -1\n",
    "        for i in range(0,15):\n",
    "            self.track_info[5,i] = -1\n",
    "        self.track_info[5,16] = -1\n",
    "        self.track_info[5,19] = -1\n",
    "        self.track_info[6,13] = -1\n",
    "        self.track_info[6,19] = -1\n",
    "        self.track_info[7,13] = -1\n",
    "        self.track_info[7,19] = -1\n",
    "        for i in range(14,19):\n",
    "            self.track_info[7,i] = 1\n",
    "       \n",
    "        self.START_STATE_1 = [2,1,0,0]\n",
    "        self.START_STATE_2 = [3,1,0,0]\n",
    "        self.START_STATE_3 = [4,1,0,0]\n",
    "        self.START_STATES = [self.START_STATE_1, self.START_STATE_2, self.START_STATE_3]\n",
    "\n",
    "    \n",
    "\n",
    "    def take_action(self, curr, action):\n",
    "        # Given a current state and next action, return the rewards and next state\n",
    "        \n",
    "        # New state in case no wall or finish line hit\n",
    "        new = []\n",
    "        new.append(curr[0] + curr[2])\n",
    "        new.append(curr[1] + curr[3])\n",
    "        new.append(curr[2] + action[0])\n",
    "        new.append(curr[3] + action[1])\n",
    "        \n",
    "    \n",
    "        # Collision detection with the wall or finish line\n",
    "        # Track the path of the car along the velocity vector and check which kind of cell is entered\n",
    "        # This algorithm has been better explained in report\n",
    "        posi = [curr[0], curr[1]]\n",
    "        posf = [new[0], new[1]]\n",
    "        path = [curr[0], curr[1]]\n",
    "        signal = self.collision_detection(posi, posf, path)\n",
    "        # Normal state-action\n",
    "        if(signal == 0): return (new, -1)\n",
    "\n",
    "        # None as next state means finish line reached\n",
    "        elif(signal == 1): return (None, -1)\n",
    "\n",
    "        # Wall hit\n",
    "        elif(signal == -1): \n",
    "            # Random reset\n",
    "            choice = int(np.random.randint(0, high=3, size=1, dtype=int))\n",
    "            return (self.START_STATES[choice], -100)\n",
    "        \n",
    "        \n",
    "    def collision_detection(self, posi, posf, path):\n",
    "\n",
    "        if(posf[0] != posi[0]):\n",
    "            slope = (posf[1] - posi[1])/(posf[0] - posi[0])\n",
    "            c = posi[1] - slope * posi[0]\n",
    "        \n",
    "        # Set of all horizontal cell borders that the velocity vector intersects\n",
    "        horizontals = np.linspace(posi[1]+0.5, posf[1] - 0.5, num = posf[1] - posi[1])\n",
    "\n",
    "        # Set of all vertical cell borders intersected\n",
    "        verticals = np.linspace(posi[0]+0.5, posf[0] - 0.5, num = posf[0] - posi[0])\n",
    "\n",
    "        \n",
    "        marker = 0  # pointer to know which verticals have been crossed already\n",
    "        signal = 0  # Wall hit, finish line or normal\n",
    "        for horz in horizontals:\n",
    "            if (marker >= verticals.shape[0]):\n",
    "                path[1] += 1\n",
    "                signal = (self.track_info[path[0], path[1]])\n",
    "                if (signal != 0):\n",
    "                    break\n",
    "            else:\n",
    "                contact_x = (horz - c)/slope\n",
    "                for i in range(marker, verticals.shape[0]):\n",
    "                    if(verticals[i] < contact_x):\n",
    "                        path[0] += 1\n",
    "                        marker = i + 1\n",
    "                        signal = (self.track_info[path[0], path[1]])\n",
    "                        if (signal != 0):\n",
    "                            break\n",
    "                        if (marker >= verticals.shape[0]):\n",
    "                            path[1] += 1\n",
    "                            signal = (self.track_info[path[0], path[1]])\n",
    "                            if (signal != 0):\n",
    "                                break\n",
    "                    elif(verticals[i] == contact_x):\n",
    "                        path[0] += 1\n",
    "                        path[1] += 1\n",
    "                        marker = i + 1\n",
    "                        signal = (self.track_info[path[0], path[1]])\n",
    "                        break\n",
    "                    else:\n",
    "                        path[1] += 1\n",
    "                        signal = (self.track_info[path[0], path[1]])\n",
    "                        break\n",
    "                if (signal != 0):\n",
    "                    break\n",
    "\n",
    "        if(signal == 0 and marker < verticals.shape[0]):\n",
    "            for i in range(marker, verticals.shape[0]):\n",
    "                path[0] += 1\n",
    "                signal = (self.track_info[path[0], path[1]])\n",
    "                if (signal != 0):\n",
    "                    break\n",
    "        \n",
    "        # Take care if signal = -1 or 1\n",
    "        return signal\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map action number to action value\n",
    "def act_map(st):\n",
    "    list = [(-1,-1), (-1,0), (-1,1), (0,-1), (0,0), (0,1), (1,-1), (1,0), (1,1)]\n",
    "    return list[int(st)]\n",
    "\n",
    "# Map state to its string form\n",
    "# \"XX-YY-Vx-Vy\" format of 6 characters\n",
    "# For example: [2,1,0,0] --> \"020100\"\n",
    "def state_map(list):\n",
    "    st = \"\"\n",
    "    i = 0\n",
    "    while(i < len(list)):\n",
    "        if (list[i] < 10 and i < 2):\n",
    "            strin = \"0\" + str(list[i])\n",
    "        else:\n",
    "            strin = str(list[i])\n",
    "        st += strin\n",
    "        i += 1\n",
    "    return st"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualiser (PyGame, Custom Made)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise(state_path):\n",
    "    ENV = Environment()\n",
    "    # Define constants\n",
    "    CELL_SIZE = 25\n",
    "    SCREEN_WIDTH = CELL_SIZE * 8\n",
    "    SCREEN_HEIGHT = CELL_SIZE * 21\n",
    "\n",
    "    # Initialize Pygame\n",
    "    pygame.init()\n",
    "\n",
    "    # Create the screen\n",
    "    screen = pygame.display.set_mode((SCREEN_WIDTH, SCREEN_HEIGHT))\n",
    "\n",
    "    # Define a function to draw the grid\n",
    "    def draw_grid(state_str, state_path):\n",
    "    # Create the grid\n",
    "        #Racetrack \n",
    "        screen.fill(0)\n",
    "        for i in range(21):\n",
    "            for j in range(8):\n",
    "                if(i ==1 and (j == 2 or j ==3 or j == 4)):\n",
    "                    color = (0,255,0)\n",
    "                    pygame.draw.rect(screen,color,((j*CELL_SIZE,i*CELL_SIZE),(CELL_SIZE,CELL_SIZE)))\n",
    "                elif (ENV.track_info[j,i]== -1 or ((j == 7 or j == 6) and (i == 20 or i <= 12))):\n",
    "                    color = (255,0,0)\n",
    "                    pygame.draw.rect(screen,color,((j*CELL_SIZE,i*CELL_SIZE),(CELL_SIZE,CELL_SIZE)))\n",
    "                elif ENV.track_info[j,i] == 0:\n",
    "                    color = (0,0,255)\n",
    "                    pygame.draw.rect(screen,color,((j*CELL_SIZE,i*CELL_SIZE),(CELL_SIZE,CELL_SIZE)))\n",
    "                elif ENV.track_info[j,i] == 1:\n",
    "                    color = (255,165,0)\n",
    "                    pygame.draw.rect(screen,color,((j*CELL_SIZE,i*CELL_SIZE),(CELL_SIZE,CELL_SIZE)))\n",
    "                pygame.draw.rect(screen,(0,0,0),((j*CELL_SIZE,i*CELL_SIZE),(CELL_SIZE,CELL_SIZE)), 1)\n",
    "            \n",
    "                    \n",
    "        #Car current position\n",
    "        x = int(state_str[:2])\n",
    "        y = int(state_str[2:4])\n",
    "        color = (255,255,255)\n",
    "        pygame.draw.rect(screen,color,((x*CELL_SIZE,y*CELL_SIZE),(CELL_SIZE,CELL_SIZE)))\n",
    "        \n",
    "        # Car start position\n",
    "        x_start = int(state_path[0][:2])\n",
    "        y_start = int(state_path[0][2:4])\n",
    "        pygame.draw.rect(screen, (0,0,0), ((x_start*CELL_SIZE,y_start*CELL_SIZE),(CELL_SIZE,CELL_SIZE)))\n",
    "        \n",
    "        # Car path\n",
    "        iter = 0\n",
    "        for state_str in state_path:\n",
    "            x = int(state_str[:2]) + 0.5\n",
    "            y = int(state_str[2:4]) + 0.5\n",
    "            if(iter != 0):\n",
    "                pygame.draw.line(screen, (255,255,0), (x_old*CELL_SIZE, y_old*CELL_SIZE), (x*CELL_SIZE, y*CELL_SIZE), 2)\n",
    "            x_old = x\n",
    "            y_old = y\n",
    "            iter += 1\n",
    "            if(iter == len(state_path)):\n",
    "                x = x_old + int(state_str[4:5])\n",
    "                y = y_old + int(state_str[5:6])\n",
    "                if(x > 7.5): x = 7.5\n",
    "                if(y > 18.5): y = y_old\n",
    "                pygame.draw.line(screen, (255,255,0), (x_old*CELL_SIZE, y_old*CELL_SIZE), (x*CELL_SIZE, y*CELL_SIZE), 2)\n",
    "    \n",
    "    # Main loop\n",
    "    running = True\n",
    "    i =0\n",
    "    while running:\n",
    "        i += 1\n",
    "        # Handle events\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                running = False\n",
    "        if(i == 2): running = False\n",
    "        for state_str in state_path:\n",
    "            # Draw the grid\n",
    "            draw_grid(state_str, state_path)\n",
    "            \n",
    "            # Update the screen\n",
    "            pygame.display.update()\n",
    "            \n",
    "            time.sleep(0.5)\n",
    "        \n",
    "        #For terminal state, as it is not included in the state_path because of the way i have coded\n",
    "        sec_last = state_path[-1]\n",
    "        sec_last_x = sec_last[:2]\n",
    "        sec_last_y = sec_last[2:4]\n",
    "        sec_last_vx = sec_last[4:5]\n",
    "        sec_last_vy = sec_last[5:6]\n",
    "        new_x = int(sec_last_x) + int(sec_last_vx)\n",
    "        new_y = int(sec_last_y) + int(sec_last_vy)\n",
    "        if(new_x > 7): new_x = 7\n",
    "        if(new_y > 18): new_y = int(sec_last_y)\n",
    "        draw_grid(\"0\" + str(new_x)+ str(new_y), state_path)\n",
    "        pygame.display.update()\n",
    "        time.sleep(0.5)\n",
    "    # Quit Pygame\n",
    "    pygame.display.quit()\n",
    "    pygame.quit()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On-Policy Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class On_Agent:\n",
    "    def __init__(self):\n",
    "        # Calculate and store possible velocities and actions possible on them\n",
    "        self.possible_actions = {}\n",
    "        for i in range(0,6):\n",
    "            for j in range(0,6):\n",
    "                list = []\n",
    "                for k in range(0,9):\n",
    "                    tup = act_map(str(k))\n",
    "                    i_new = i + tup[0]\n",
    "                    j_new = j + tup[1]\n",
    "                    if((i_new <= 5) and (i_new >= 0) and (j_new <= 5) and (j_new >= 0) and not (i_new == 0 and j_new == 0)):\n",
    "                        list.append(str(k))\n",
    "                self.possible_actions[str(i) + str(j)] = (len(list), list)\n",
    "\n",
    "        \n",
    "        self.policy = {}    # Greedy Policy\n",
    "        self.state_action_vals = {} # Q-values\n",
    "        self.first_visits = {}  # First-visit records for one episode\n",
    "    \n",
    "\n",
    "    def decide_action(self, curr_state_str, epsilon):\n",
    "        # Decide epsilon-soft action using data from greedy policy\n",
    "        action = \"\"\n",
    "        ep = epsilon\n",
    "        poss_act = self.possible_actions[curr_state_str[len(curr_state_str) - 2:]]\n",
    "        num_act = poss_act[0]   #Number of possible actions for given state\n",
    "        act_lst = poss_act[1]   #Set of possible actions for given state\n",
    "        choice = int(np.random.randint(0, high=num_act, size=1, dtype=int))\n",
    "        \n",
    "        # Tossing an epsilon-biased coin\n",
    "        if (np.random.rand() < ep):\n",
    "            action = act_lst[choice]\n",
    "        \n",
    "        else:\n",
    "            # Greedy action\n",
    "            action = self.policy.get(curr_state_str)\n",
    "\n",
    "            # If greedy action exists but there is a possible action that hasnt been tried for the state, choose that\n",
    "            if(action != None and self.state_action_vals.get(curr_state_str + \"done\") == 0):\n",
    "                action = act_lst[choice]\n",
    "\n",
    "            # If greedy action not yet decided\n",
    "            if(action == None):\n",
    "                action = act_lst[choice]\n",
    "                self.policy[curr_state_str] = action\n",
    "        return action\n",
    "    \n",
    "\n",
    "    def episode(self, env, epsilon, print_path = 0):\n",
    "        curr_state = env.START_STATES[int(np.random.randint(0, high=3, size=1, dtype=int))]\n",
    "        \n",
    "        count = 0\n",
    "        self.first_visits.clear()\n",
    "        state_list = []\n",
    "        wall_list = []\n",
    "        while(curr_state != None):\n",
    "            \n",
    "            curr_state_str = state_map(curr_state)\n",
    "            state_list.append(curr_state_str)\n",
    "            # Decide epsilon-soft action using data from greedy policy\n",
    "            ep = epsilon\n",
    "            action = self.decide_action(curr_state_str, ep)\n",
    "            \n",
    "            # Find step number if this is first visit \n",
    "            if self.first_visits.get(curr_state_str + action) == None :\n",
    "                self.first_visits[curr_state_str + action] = [count, 0]\n",
    "\n",
    "            # Get next state and reward\n",
    "            ret_tup = (env.take_action(curr_state, act_map(action)))\n",
    "            curr_state = ret_tup[0]\n",
    "            curr_reward = ret_tup[1]\n",
    "            count += 1\n",
    "\n",
    "            #If there was a wall hit\n",
    "            if(curr_reward == -100): wall_list.append(1)\n",
    "            else: wall_list.append(0)\n",
    "        \n",
    "        \n",
    "        if(print_path != 1):\n",
    "            return (count, wall_list)\n",
    "        elif(print_path == 1):\n",
    "            return (count, wall_list, state_list)\n",
    "\n",
    "    def calc_return(self, state_act, episode_info, gamma):\n",
    "        state_act_ret = 0\n",
    "        \n",
    "        # Return from timesteps\n",
    "        ret_num = -(self.first_visits[state_act][0] - episode_info[0])\n",
    "        state_act_ret = (1- pow(gamma, ret_num))/(1- gamma)\n",
    "        \n",
    "        # Return from Wall hits\n",
    "        for i in range(self.first_visits[state_act][0], len(episode_info[1])):\n",
    "            if(episode_info[1][i] == 1):\n",
    "                diff = i - self.first_visits[state_act][0]\n",
    "                state_act_ret += pow(gamma, diff) * -10\n",
    "        \n",
    "        return state_act_ret\n",
    "\n",
    "\n",
    "    def update_values_policy(self, state_act, state_act_ret):\n",
    "        # After an episode, update the state-action values and decide greedy actions\n",
    "        \n",
    "        # Update the state-action value\n",
    "        curr_set = self.state_action_vals.get(state_act)\n",
    "        if curr_set == None:\n",
    "            self.state_action_vals[state_act] = [1,state_act_ret]\n",
    "        else:\n",
    "            self.state_action_vals[state_act] = [curr_set[0] + 1, (curr_set[1]*curr_set[0] + state_act_ret)/(curr_set[0] + 1)]\n",
    "        \n",
    "        # Find the new greedy action for given state\n",
    "        updated_val = self.state_action_vals[state_act][1]\n",
    "        state_str = state_act[:6]\n",
    "        act_str = state_act[6:]\n",
    "        state_max = self.state_action_vals.get(state_str + \"max\")\n",
    "        \n",
    "        # If another action has got more value.\n",
    "        if(state_max == None or state_max[1] < updated_val):\n",
    "            self.state_action_vals[state_str + \"max\"] = [act_str, updated_val]\n",
    "            curr_act = self.policy.get(state_str)\n",
    "            if(curr_act == None or curr_act != act_str):\n",
    "                self.policy[state_str] = act_str\n",
    "                \n",
    "        \n",
    "        # If the value of previously greedy action has decreased\n",
    "        elif(state_max[0] == act_str):\n",
    "            for poss_act in self.possible_actions[state_str[4:]][1]:\n",
    "                temp = self.state_action_vals.get(state_str + poss_act)\n",
    "                if (temp != None and updated_val < temp[1]):\n",
    "                    self.state_action_vals[state_str + \"max\"] = [poss_act, temp[1]]\n",
    "                    curr_act = self.policy.get(state_str)\n",
    "                    if(curr_act == None or curr_act != poss_act):\n",
    "                        self.policy[state_str] = poss_act\n",
    "                        \n",
    "        \n",
    "        #If there is an action which has not been explored for given state\n",
    "        if(self.state_action_vals.get(state_str + \"done\") == None or self.state_action_vals.get(state_str + \"done\") == 0):\n",
    "            mark = 0\n",
    "            for poss_act in self.possible_actions[state_str[4:]][1]:\n",
    "                temp = self.state_action_vals.get(state_str + poss_act)\n",
    "                if(temp == None):\n",
    "                    mark = 1\n",
    "                    self.state_action_vals[state_str + poss_act] = [0,0]\n",
    "                    self.state_action_vals[state_str + \"max\"] = [poss_act, 0]\n",
    "                    curr_act = self.policy.get(state_str)\n",
    "                    if(curr_act == None or curr_act != poss_act):\n",
    "                        self.policy[state_str] = poss_act\n",
    "                        \n",
    "                    break\n",
    "            if(mark == 0):\n",
    "                self.state_action_vals[state_str + \"done\"] = 1\n",
    "        \n",
    "        return\n",
    "\n",
    "    def evaluate_episode(self, episode_info, gamma):\n",
    "        for state_act in self.first_visits:\n",
    "            state_act_ret = self.calc_return(state_act, episode_info, gamma)\n",
    "            self.update_values_policy(state_act, state_act_ret)\n",
    "            \n",
    "        return\n",
    "    \n",
    "\n",
    "    def learn(self, env, epsilons):\n",
    "        \"\"\" self.list1 = []\n",
    "        self.list2 = [] \"\"\"\n",
    "        for epsilon in epsilons:\n",
    "            epsil = 0.6\n",
    "            i = 0\n",
    "            while(i < epsilon[1]):\n",
    "                i += 1\n",
    "                if(i > 50): epsil = epsilon[0]\n",
    "                ep_len = self.episode(env, epsil)\n",
    "                gamma = 0.9\n",
    "                self.evaluate_episode(ep_len[:2], gamma)\n",
    "                \"\"\" self.list1.append(None if self.state_action_vals.get(\"0201005\") == None else self.state_action_vals.get(\"0201005\")[1])\n",
    "                self.list2.append(None if self.state_action_vals.get(\"0301005\") == None else self.state_action_vals.get(\"0301005\")[1]) \"\"\"\n",
    "        \"\"\" plt.plot(agent.list1)\n",
    "            plt.show() \"\"\"\n",
    "        return\n",
    "    \n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def On_policy():\n",
    "    #For learning\n",
    "    env = Environment()\n",
    "    agent = On_Agent()\n",
    "    epsilons = [(0.3, 120)]\n",
    "    agent.learn(env, epsilons)\n",
    "    return agent\n",
    "\n",
    "def On_policy_test(agent):\n",
    "    #For visualisation\n",
    "    env = Environment()\n",
    "    state_path = agent.episode(env, 0, 1)[2]\n",
    "    visualise(state_path)\n",
    "    return\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Off-Policy Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Off_Agent:\n",
    "    def __init__(self):\n",
    "        # Calculate and store possible velocities and actions possible on them\n",
    "        self.possible_actions = {}\n",
    "        for i in range(0,6):\n",
    "            for j in range(0,6):\n",
    "                list = []\n",
    "                for k in range(0,9):\n",
    "                    tup = act_map(str(k))\n",
    "                    i_new = i + tup[0]\n",
    "                    j_new = j + tup[1]\n",
    "                    if((i_new <= 5) and (i_new >= 0) and (j_new <= 5) and (j_new >= 0) and not (i_new == 0 and j_new == 0)):\n",
    "                        list.append(str(k))\n",
    "                self.possible_actions[str(i) + str(j)] = (len(list), list)\n",
    "\n",
    "        \n",
    "        self.policy = {}    # Greedy Policy\n",
    "        self.state_action_vals = {} # Q-values\n",
    "        self.every_visits = []  # every-visit records for one episode\n",
    "    \n",
    "\n",
    "    def episode(self, env, epsilon, print_path = 0):\n",
    "        curr_state = env.START_STATES[int(np.random.randint(0, high=3, size=1, dtype=int))]\n",
    "        count = 0\n",
    "        self.every_visits.clear()\n",
    "        state_list = []\n",
    "        wall_list = []\n",
    "        prob_list = [] #For getting the importance sampling weights\n",
    "        while(curr_state != None):\n",
    "            \n",
    "            curr_state_str = state_map(curr_state)\n",
    "            state_list.append(curr_state_str)\n",
    "            #########################################################################################\n",
    "            # Behaviour Policy\n",
    "            # Decide epsilon-soft action using data from greedy policy\n",
    "            action = \"\"\n",
    "            ep = epsilon\n",
    "            poss_act = self.possible_actions[curr_state_str[len(curr_state_str) - 2:]]\n",
    "            num_act = poss_act[0]   #Number of possible actions for given state\n",
    "            act_lst = poss_act[1]   #Set of possible actions for given state\n",
    "            choice = int(np.random.randint(0, high=num_act, size=1, dtype=int))\n",
    "            \n",
    "            # Tossing an epsilon-biased coin\n",
    "            if (np.random.rand() < ep):\n",
    "                action = act_lst[choice]\n",
    "                prob_list.append(ep/num_act)\n",
    "            \n",
    "            else:\n",
    "                # Greedy action\n",
    "                action = self.policy.get(curr_state_str)\n",
    "                # If greedy action not yet decided\n",
    "                if(action == None):\n",
    "                    action = act_lst[choice]\n",
    "                    self.policy[curr_state_str] = action\n",
    "                    prob_list.append(1/num_act)\n",
    "                else: prob_list.append(1 - ep + ep/num_act)\n",
    "            #############################################################################################\n",
    "            \n",
    "            #Update every visit list\n",
    "            self.every_visits.append(curr_state_str + action)\n",
    "\n",
    "            # Get next staet and reward\n",
    "            ret_tup = (env.take_action(curr_state, act_map(action)))\n",
    "            curr_state = ret_tup[0]\n",
    "            curr_reward = ret_tup[1]\n",
    "            count += 1\n",
    "\n",
    "            # If a wall hit was involved\n",
    "            if(curr_reward == -100): wall_list.append(1)\n",
    "            else: wall_list.append(0)\n",
    "\n",
    "        if(print_path != 1):\n",
    "            return (count, wall_list, prob_list)\n",
    "        elif(print_path == 1):\n",
    "            return (count, wall_list, prob_list, state_list)\n",
    "\n",
    "    def evaluate_episode(self, episode_length, gamma):\n",
    "        # Evaluate and update state-action values and target policy after an episode\n",
    "        state_act_ret = 0\n",
    "        state_act_W = 1\n",
    "        for index, state_act in reversed(list(enumerate(self.every_visits))):\n",
    "            # Timestep reward\n",
    "            state_act_ret += -1 + gamma * (state_act_ret)\n",
    "\n",
    "            #If wall hit occurred\n",
    "            if episode_length[1][index] == 1:\n",
    "                state_act_ret += -10\n",
    "           \n",
    "            # Update state-action value\n",
    "            curr_set = self.state_action_vals.get(state_act)\n",
    "            if curr_set == None:\n",
    "                self.state_action_vals[state_act] = [state_act_W, state_act_ret]\n",
    "            else:\n",
    "                self.state_action_vals[state_act] = [curr_set[0] + state_act_W, curr_set[1] + state_act_W * (state_act_ret - curr_set[1])/(curr_set[0] + state_act_W)]\n",
    "            \n",
    "            updated_val = self.state_action_vals[state_act][1]\n",
    "            state_str = state_act[:6]\n",
    "            act_str = state_act[6:]\n",
    "            state_max = self.state_action_vals.get(state_str + \"max\")\n",
    "            \n",
    "            # Update max stat-action values and target policy\n",
    "            if(state_max == None or state_max[1] < updated_val):\n",
    "                self.state_action_vals[state_str + \"max\"] = [act_str, updated_val]\n",
    "                curr_act = self.policy.get(state_str)\n",
    "                if(curr_act == None or curr_act != act_str):\n",
    "                    self.policy[state_str] = act_str\n",
    "\n",
    "            elif(state_max[0] == act_str):\n",
    "                for poss_act in self.possible_actions[state_str[4:]][1]:\n",
    "                    temp = self.state_action_vals.get(state_str + poss_act)\n",
    "                    if (temp != None and updated_val < temp[1]):\n",
    "                        self.state_action_vals[state_str + \"max\"] = [poss_act, temp[1]]\n",
    "                        curr_act = self.policy.get(state_str)\n",
    "                        if(curr_act == None or curr_act != poss_act):\n",
    "                            self.policy[state_str] = poss_act\n",
    "                            \n",
    "            #Check if next W is going to be 0 or not, if W- 0 it means the trajectory probabiliy is zero\n",
    "            if(act_str != self.policy[state_str]): break\n",
    "            \n",
    "            #Update Relative trajectory probability (weight)\n",
    "            state_act_W /= episode_length[2][index]\n",
    "        return\n",
    "    \n",
    "\n",
    "    def learn(self, env, epsilons):\n",
    "        \n",
    "        for epsilon in epsilons:\n",
    "            \"\"\" self.list1 = []\n",
    "            self.list2 = [] \"\"\"\n",
    "            epsil = 0.3\n",
    "            i = 0\n",
    "            while(i < epsilon[1]):\n",
    "                i += 1\n",
    "                if(i > 40000): epsil = epsilon[0]\n",
    "                ep_len = self.episode(env, epsil)\n",
    "                gamma = 0.9\n",
    "                self.evaluate_episode(ep_len[:3], gamma)\n",
    "                \"\"\" self.list1.append(None if self.state_action_vals.get(\"0201005\") == None else self.state_action_vals.get(\"0201005\")[1])\n",
    "                self.list2.append(None if self.state_action_vals.get(\"0301005\") == None else self.state_action_vals.get(\"0301005\")[1]) \"\"\"\n",
    "        \"\"\" plt.plot(agent.list1)\n",
    "            plt.show() \"\"\"\n",
    "        return\n",
    "    \n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Off_policy():\n",
    "    env = Environment()\n",
    "    agent = Off_Agent()\n",
    "    epsilons = [(0.3, 50000)]\n",
    "    agent.learn(env, epsilons)\n",
    "    return agent\n",
    "\n",
    "def Off_policy_test(agent):\n",
    "    env = Environment()\n",
    "    state_path = agent.episode(env, 0, 1)[3]\n",
    "    visualise(state_path)\n",
    "    return\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Agents"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On-Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train agent, run this first and then visualise\n",
    "On_agent = On_policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Visualisation, run multiple times to see paths from different start posiitons\n",
    "On_policy_test(On_agent)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Off-Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train agent, run this first and then visualise\n",
    "Off_Agent = Off_policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Visualisation, run multiple times to see paths from different start posiitons\n",
    "Off_policy_test(Off_Agent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sklearn-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8f887bcd234464783ee13182c02ebbb8cc68ab94ebb94fdfc49a0f0ced571bfe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
